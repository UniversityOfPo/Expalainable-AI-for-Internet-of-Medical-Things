# Importance of this research:
The importance of this research lies in its ability to address the critical need for accurate and interpretable AI-driven diagnostics in healthcare, particularly for complex conditions like brain tumors. This framework improves the accuracy and reliability of AI predictions by using advanced XAI techniques like LIME, SHAP, and Grad-CAM along with a strong majority voting ensemble model. This builds trust among healthcare professionals. The innovative use of cloud-edge architectures allows for efficient handling of large datasets and computational workloads, making real-time, interpretable diagnostics feasible in clinical settings. Supported by the National Science Foundation, this research not only improves diagnostic accuracy but also advances the ethical application of AI in healthcare, ensuring decisions are transparent and understandable for practitioners.

# Abstract:
The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems. With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial. Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT. The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications. Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system. Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demonstrating accurate and transparent diagnosis. Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%. Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT.

# Research paper URL: https://ieeexplore.ieee.org/abstract/document/10464798
# Funded By: National
Science Foundation Award Numbers 2205773 and 2219658.

# Bib TeX:

@INPROCEEDINGS{10464798,
  author={Amin, Al and Hasan, Kamrul and Zein-Sabatto, Saleh and Chimba, Deo and Ahmed, Imtiaz and Islam, Tariqul},
  booktitle={2023 IEEE Globecom Workshops (GC Wkshps)}, 
  title={An Explainable AI Framework for Artificial Intelligence of Medical Things}, 
  year={2023},
  volume={},
  number={},
  pages={2097-2102},
  keywords={Training;Industries;Explainable AI;Instruments;Refining;Medical services;Real-time systems;Explainable AI (XAI);Maximum Voting Classifier;Internet of Medical Things;Intelligent Healthcare System;Health},
  doi={10.1109/GCWkshps58843.2023.10464798}}


# For more information:
AL AMIN (aamin2@my.tnstate.edu). PhD Student, Tennessee State University.
Dr. Kamrul Hasan, Assistant Professor, Tennessee State University (mhasan1@tnstate.edu)

